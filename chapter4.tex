\chapter{Classification}
In classification task, we find a model for class attribute as a function of the values of other attributes. The goal is to assign previously unseen records a class as accurately as possible.

\section{Decision Tree based}

\subsection{Hunt's Algorithm}

Hunt's algorithm grows a decision tree in a recursive fashion by partitioning the training records into successively purer subsets. Let $D_t$ be the set of training records that reach a node $t$:

\begin{itemize}
\item If $D_t$ contains records that belong the same class $y_t$, then $t$ is a leaf node labeled as $y_t$
\item If $D_t$ is an empty set, then t is a leaf node labeled by the default class
\item If $D_t$ contains records that belong to more than one class, use an attribute test to split the data into smaller subsets.
\end{itemize}

\subsection{Measure of Node Impurity}

\underline{Gini Index}

$$GINI(t) = 1 - \sum_{j} P(j\mid t)^{2}$$

$$GINI_{split}=\sum_{i=1}^{k} \frac{n_i}{n} GINI(i)$$
\noindent
\underline{Entrophy} 

$$Entrophy(t)=-\sum P(j \mid t) log_2 P(j \mid t)$$ 

$$GAIN_{split}=Entrophy(p)-\sum_{i=1}^{k} \frac{n_i}{n}Entrophy(i)$$
\noindent
Disadvantage: Tends to prefer splits that result in large number of partitions, each being small but pure. \\
\par \noindent
Introduce Gain Ratio: 

$$GainRATIO_{split}=\frac{GAIN_{split}}{SplitINFO}$$

$$SplitINFO=-\sum_{i=1}^{k} \frac{n_i}{n}log_2 \frac{n_i}{n}$$ \\
\underline{Misclassification Error}

$$Error(t)=1-max_i P(i \mid t)$$ 
\par \noindent
{\it Hint: To compute maximum value for Gini Index, Entrophy, or Misclassification Error, subtitute $P(j\mid t)$ with $1/ n_c$. Minimum values are always 0. }

\section{Rule-based}
Classify records by using a collection of "if… then…" rules. \\

\subsection{Rule Coverage and Accuracy}
Given a rule $r$ in a dataset $D$:
$$r: A \rightarrow y$$

$$coverage=\frac{\mid A \mid}{\mid D \mid}$$
$$accuracy=\frac{\mid A \cap y \mid}{\mid A \mid}$$

\subsection{Characteristics of Rule-Based Classifier}

\begin{description}
  \item[Mutually exclusive] Every record is covered by at most one rule. No two rules are trigger by the same record.
  \item[Exhaustive] Each record is covered by at least one rule
\end{description}

\section{Model Evaluation}

Focus on predictive capability of a model

\subsection{Metrics for Performance Evaluation}

Confusion Matrix:
\begin{center}
\begin{tabular}{ | p{4cm} | p{4cm} | p{4cm} | } 
    \hline
    Count & Predicted Class=YES & Predicted Class=NO \\
    \hline
    Actual Class = YES & a(TP) & b(FN) \\
    \hline
    Actual Class = NO & c(FP) & d (TN) \\
    \hline
\end{tabular}
\end{center}

$$Accuracy=\frac{a + d}{a + b + c + d}$$

$$Weighted\ Accuracy=\frac{w_1a + w_4d}{w_1a + w_2b + w_3c + w_4d}$$

$$Precision,p=\frac{a}{a + c}$$

$$Recall,r=\frac{a}{a+b}$$

$$F-measure,f=\frac{2a}{2a + b + c}$$

\subsection{Methods for Performance Evaluation}
\begin{description}
\item[Holdout] reserve 2/3 for training and 1/3 for testing
\item[Random subsampling] repeated holdout
\item[Cross validation] partition data into k disjoint subsets
\item[Stratified sampling] oversampling vs undersampling
\item[Bootstrap] sampling with replacement
\end{description}

\subsection{Methods for Model Comparison}
ROC curve is a graph of TP rate againts FP rate
$$TP\ rate, TPR = \frac{a}{a+b}$$
$$FP\ rate, FPR = \frac{c}{c + d}$$


